---
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:${CP_VERSION}
    hostname: zookeeper
    restart: always
    ports:
      - "2181:2181"
    volumes:
      - data-zookeeper-log:/var/lib/zookeeper/log
      - data-zookeeper-data:/var/lib/zookeeper/data
      - ${PWD}/jaas/zookeeper.config:/etc/zookeeper/secrets/zookeeper-server-jaas.config
    environment:
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_JMX_HOSTNAME: zookeeper
      ZOOKEEPER_CLIENT_PORT: 2181
      KAFKA_OPTS: -Dzookeeper.4lw.commands.whitelist=*
        -Djava.security.auth.login.config=/etc/zookeeper/secrets/zookeeper-server-jaas.config
        -Dzookeeper.authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider
        -Dzookeeper.allowSaslFailedClients=false
        -Dzookeeper.requireClientAuthScheme=sasl
 
  broker:
    image: confluentinc/cp-server:${CP_VERSION}
    hostname: broker
    depends_on:
      - zookeeper
      - broker-auditlog
    ports:
      - "19092:19092"
    volumes:
      - data-broker:/var/lib/kafka/data
      - ${PWD}/jaas/broker.config:/etc/kafka/secrets/kafka-client-jaas.config
      - ./examples:/examples
      - ./clients:/clients
    healthcheck:
      test: kafka-topics --command-config /clients/admin.conf --list --bootstrap-server broker:9092 || exit 1
      interval: 1s
      timeout: 60s
      retries: 60
    environment:
      KAFKA_BROKER_ID: 1
      # Only for debugging:
      KAFKA_LOG4J_LOGGERS: DEBUG
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_REPORTERS_TELEMETRY_AUTO_ENABLE: 'false'
      KAFKA_CONFLUENT_TELEMETRY_ENABLED: 'false'
      KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE: "false"
      KAFKA_CONFLUENT_CLUSTER_LINK_ENABLE: "false"
      KAFKA_CONFLUENT_BALANCER_ENABLE: "false"
      # metrics (disabled)
      CONFLUENT_METRICS_ENABLE: "false"
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
 
      # listeners
      KAFKA_ADVERTISED_LISTENERS: CLIENTS://broker:9092,BROKER://broker:9093,HOST://localhost:19092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CLIENTS:SASL_PLAINTEXT,BROKER:SASL_PLAINTEXT,HOST:SASL_PLAINTEXT
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_SUPER_USERS: "User:broker;User:admin"
      # listener "CLIENTS"
      KAFKA_LISTENER_NAME_CLIENTS_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_NAME_CLIENTS_PLAIN_SASL_JAAS_CONFIG: |-
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        username="broker" \
        password="broker-secret" \
        user_broker="broker-secret" \
        user_producer="producer-secret" \
        user_consumer="consumer-secret" \
        user_admin="admin-secret";
      # listener "BROKER"
      KAFKA_LISTENER_NAME_BROKER_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_NAME_BROKER_PLAIN_SASL_JAAS_CONFIG: |-
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        username="broker" \
        password="broker-secret" \
        user_broker="broker-secret";
      # listener "HOST"
      KAFKA_LISTENER_NAME_HOST_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_NAME_HOST_PLAIN_SASL_JAAS_CONFIG: |-
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        username="broker" \
        password="broker-secret" \
        user_broker="broker-secret" \
        user_producer="producer-secret" \
        user_consumer="consumer-secret" \
        user_admin="admin-secret";
      # audit logger
      CONFLUENT_SECURITY_EVENT_LOGGER_EXPORTER_KAFKA_SASL_JAAS_CONFIG: |-
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        username="broker" \
        password="broker-secret";
      # Confluent ACL-based authorizer
      KAFKA_AUTHORIZER_CLASS_NAME: io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "false"
      # confluent audit logs
      # These lines could be configured dynamically (to be more concise with the subject of this demo), too.
      # But for simplicity, we configuring them statically here. Please leave uncommented
      KAFKA_CONFLUENT_SECURITY_EVENT_LOGGER_ENABLE: 'true'
      # Do not log authentication for now. Set to 'true' to enable this for testing
      KAFKA_CONFLUENT_SECURITY_EVENT_LOGGER_AUTHENTICATION_ENABLE: 'true'
      # Configuration for the audit log exporter which will write audit log events to a separate cluster
      KAFKA_CONFLUENT_SECURITY_EVENT_LOGGER_EXPORTER_KAFKA_SASL_MECHANISM: PLAIN
      KAFKA_CONFLUENT_SECURITY_EVENT_LOGGER_EXPORTER_KAFKA_SASL_JAAS_CONFIG: |-
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        username="auditlogproducer" \
        password="auditlogproducer-secret";
      # The following lines demonstrate how you could configure logging of audit log events from this cluster to the separate audit log cluster statically
      # However, im this demo we want to enable logging dynamically (see README.adoc), there these lines are commented
      KAFKA_CONFLUENT_SECURITY_EVENT_ROUTER_CONFIG: |-
        { \
          "destinations": { \
              "bootstrap_servers": [ \
                  "broker-auditlog:9092" \
              ], \
              "topics": { \
                  "confluent-audit-log-events": { \
                      "retention_ms": 31536000000 \
                  } \
              } \
          }, \
          "excluded_principals": ["User:broker", "User:admin"], \
          "default_topics": { \
              "allowed": "confluent-audit-log-events", \
              "denied": "confluent-audit-log-events" \
          }, \
          "routes": { \
              "crn:///kafka=*/topic=*": { \
                  "management": { \
                      "allowed": "confluent-audit-log-events", \
                      "denied": "confluent-audit-log-events" \
                  }, \
                  "describe": { \
                      "allowed": "confluent-audit-log-events", \
                      "denied": "confluent-audit-log-events" \
                  }, \
                  "produce": { \
                      "allowed": "confluent-audit-log-events", \
                      "denied": "confluent-audit-log-events" \
                  }, \
                  "consume": { \
                      "allowed": "confluent-audit-log-events", \
                      "denied": "confluent-audit-log-events" \
                  } \
              } \
          } \
        }
      # Alternative audit log routing configuration, for demonstrating how events could be sent to separate topics based on their type.
      # Please enable either the routing configuration above (required for demonstrating ELK!), or the one below (disabled by default)
      # KAFKA_CONFLUENT_SECURITY_EVENT_ROUTER_CONFIG: |-
      #   { \
      #     "destinations": { \
      #         "bootstrap_servers": [ \
      #             "broker-auditlog:9092" \
      #         ], \
      #         "topics": { \
      #             "confluent-audit-log-events": { \
      #                 "retention_ms": 31536000000 \
      #             }, \
      #             "confluent-audit-log-events-management": { \
      #                 "retention_ms": 31536000000 \
      #             }, \
      #             "confluent-audit-log-events-describe": { \
      #                 "retention_ms": 31536000000 \
      #             }, \
      #             "confluent-audit-log-events-produce": { \
      #                 "retention_ms": 31536000000 \
      #             }, \
      #             "confluent-audit-log-events-consume": { \
      #                 "retention_ms": 31536000000 \
      #             }
      #         } \
      #     }, \
      #     "excluded_principals": ["User:broker", "User:admin"], \
      #     "default_topics": { \
      #         "allowed": "", \
      #         "denied": "confluent-audit-log-events" \
      #     }, \
      #     "routes": { \
      #         "crn:///kafka=*/topic=*": { \
      #             "management": { \
      #                 "allowed": "", \
      #                 "denied": "confluent-audit-log-events-management" \
      #             }, \
      #             "describe": { \
      #                 "allowed": "", \
      #                 "denied": "confluent-audit-log-events-describe" \
      #             }, \
      #             "produce": { \
      #                 "allowed": "", \
      #                 "denied": "confluent-audit-log-events-produce" \
      #             }, \
      #             "consume": { \
      #                 "allowed": "", \
      #                 "denied": "confluent-audit-log-events-consume" \
      #             } \
      #         } \
      #     } \
      #   }
      # Precondition for being able to update passwords dynamically: Set an encoder secret
      KAFKA_PASSWORD_ENCODER_SECRET: "highly-confidential-0815"
      # security zookeeper
      KAFKA_OPTS: -Djava.security.auth.login.config=/etc/kafka/secrets/kafka-client-jaas.config
      # license configuration
      KAFKA_CONFLUENT_LICENSE_SECURITY_PROTOCOL: SASL_PLAINTEXT
      KAFKA_CONFLUENT_LICENSE_SASL_JAAS_CONFIG: |-
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        username="broker" \
        password="broker-secret";

  init_cluster:
    image: confluentinc/cp-server:${CP_VERSION}
    hostname: init_cluster
    container_name: init_cluster
    depends_on:
      broker:
        condition: service_healthy
    deploy: 
      restart_policy: 
        condition: none
    volumes:
     - ./init/init_cluster.sh:/usr/bin/init_cluster.sh
     - ./clients:/clients
    entrypoint: /bin/bash
    command:
      - "/usr/bin/init_cluster.sh"
    environment:
      BOOTSTRAP_SERVER: broker:9092
      COMMAND_CONFIG_FILE: /clients/admin.conf

  # schema-registry:
  #   image: confluentinc/cp-schema-registry:${CP_VERSION}
  #   hostname: schema-registry
  #   #container_name: schema-registry
  #   depends_on:
  #     - zookeeper
  #     - broker
  #   ports:
  #     - '8081:8081'
  #   environment:
  #     SCHEMA_REGISTRY_HOST_NAME: schema-registry
  #     SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker:9092'
  #     SCHEMA_REGISTRY_LISTENERS: "http://0.0.0.0:8081"
  #     SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'

  zookeeper-auditlog:
    image: confluentinc/cp-zookeeper:${CP_VERSION}
    hostname: zookeeper-auditlog
    #profiles: [auditlog]
    restart: always
    #ports:
    #  - "12181:2181"
    volumes:
      - data-zookeeper-auditlog-log:/var/lib/zookeeper/log
      - data-zookeeper-auditlog-data:/var/lib/zookeeper/data
      - ${PWD}/jaas/zookeeper.config:/etc/zookeeper/secrets/zookeeper-server-jaas.config
    environment:
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_JMX_HOSTNAME: zookeeper-auditlog
      ZOOKEEPER_CLIENT_PORT: 2181
      KAFKA_OPTS: -Dzookeeper.4lw.commands.whitelist=*
        -Djava.security.auth.login.config=/etc/zookeeper/secrets/zookeeper-server-jaas.config
        -Dzookeeper.authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider
        -Dzookeeper.allowSaslFailedClients=false
        -Dzookeeper.requireClientAuthScheme=sasl
 
  broker-auditlog:
    image: confluentinc/cp-server:${CP_VERSION}
    hostname: broker-auditlog
    #profiles: [auditlog]
    depends_on:
      - zookeeper-auditlog
    ports:
      - "29092:29092"
    volumes:
      - data-broker-auditlog:/var/lib/kafka/data
      - ${PWD}/jaas/broker.config:/etc/kafka/secrets/kafka-client-jaas.config
      - ./clients:/clients
    healthcheck:
      test: kafka-topics --command-config /clients/admin-auditlog.conf --list --bootstrap-server broker-auditlog:9092 || exit 1
      interval: 1s
      timeout: 60s
      retries: 60
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper-auditlog:2181'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_REPORTERS_TELEMETRY_AUTO_ENABLE: 'false'
      KAFKA_CONFLUENT_TELEMETRY_ENABLED: 'false'
      KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE: "false"
      KAFKA_CONFLUENT_CLUSTER_LINK_ENABLE: "false"
      KAFKA_CONFLUENT_BALANCER_ENABLE: "false"
      # metrics (disabled)
      CONFLUENT_METRICS_ENABLE: "false"
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
 
      # listeners
      KAFKA_ADVERTISED_LISTENERS: CLIENTS://broker-auditlog:9092,BROKER://broker-auditlog:9093,HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CLIENTS:SASL_PLAINTEXT,BROKER:SASL_PLAINTEXT,HOST:SASL_PLAINTEXT
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_SUPER_USERS: "User:broker;User:admin"
      # listener "CLIENTS"
      KAFKA_LISTENER_NAME_CLIENTS_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_NAME_CLIENTS_PLAIN_SASL_JAAS_CONFIG: |-
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        username="broker" \
        password="broker-secret" \
        user_broker="broker-secret" \
        user_auditlogproducer="auditlogproducer-secret" \
        user_auditlogconsumer="auditlogconsumer-secret" \
        user_admin="admin-secret";
      # listener "BROKER"
      KAFKA_LISTENER_NAME_BROKER_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_NAME_BROKER_PLAIN_SASL_JAAS_CONFIG: |-
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        username="broker" \
        password="broker-secret" \
        user_broker="broker-secret";
      # listener "HOST"
      KAFKA_LISTENER_NAME_HOST_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_NAME_HOST_PLAIN_SASL_JAAS_CONFIG: |-
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        username="broker" \
        password="broker-secret" \
        user_broker="broker-secret" \
        user_auditlogproducer="auditlogproducer-secret" \
        user_auditlogconsumer="auditlogconsumer-secret" \
        user_admin="admin-secret";
      # audit logger disabled as this is the audit log cluster
      KAFKA_CONFLUENT_SECURITY_EVENT_LOGGER_ENABLE: 'false'
      # confluent authorizer based on ACLs
      KAFKA_AUTHORIZER_CLASS_NAME: io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "false"
      # security zookeeper
      KAFKA_OPTS: -Djava.security.auth.login.config=/etc/kafka/secrets/kafka-client-jaas.config
      # license configuration
      KAFKA_CONFLUENT_LICENSE_SECURITY_PROTOCOL: SASL_PLAINTEXT
      KAFKA_CONFLUENT_LICENSE_SASL_JAAS_CONFIG: |-
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        username="broker" \
        password="broker-secret";

  init_cluster_auditlog:
    image: confluentinc/cp-server:${CP_VERSION}
    hostname: init_cluster_auditlog
    container_name: init_cluster_auditlog
    depends_on:
      broker-auditlog:
        condition: service_healthy
    deploy: 
      restart_policy: 
        condition: none
    volumes:
     - ./init/init_cluster_auditlog.sh:/usr/bin/init_cluster_auditlog.sh
     - ./clients:/clients
    entrypoint: /bin/bash
    command:
      - "/usr/bin/init_cluster_auditlog.sh"
    environment:
      BOOTSTRAP_SERVER: broker-auditlog:9092
      COMMAND_CONFIG_FILE: /clients/admin.conf

  setup:
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    profiles:
      - elastic
    user: "0"
    command: >
      bash -c '
        if [ x${ELASTIC_PASSWORD} == x ]; then
          echo "Set the ELASTIC_PASSWORD environment variable in the .env file";
          exit 1;
        elif [ x${KIBANA_PASSWORD} == x ]; then
          echo "Set the KIBANA_PASSWORD environment variable in the .env file";
          exit 1;
        fi;
        echo "Waiting for Elasticsearch availability";
        until curl -s http://es01:9200 | grep -q "missing authentication credentials"; do sleep 30; done;
        echo "Setting kibana_system password";
        until curl -s -X POST -u "elastic:${ELASTIC_PASSWORD}" -H "Content-Type: application/json" http://es01:9200/_security/user/kibana_system/_password -d "{\"password\":\"${KIBANA_PASSWORD}\"}" | grep -q "^{}"; do sleep 10; done;
        echo "All done!";
      '

  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    hostname: es01
    labels:
      co.elastic.logs/module: elasticsearch
    volumes:
     - esdata01:/usr/share/elasticsearch/data
    ports:
      - ${ES_PORT}:9200
    environment:
      - node.name=es01
      - cluster.name=${CLUSTER_NAME}
      - discovery.type=single-node
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=false
      - xpack.security.transport.ssl.enabled=false
      - xpack.license.self_generated.type=${LICENSE}
    mem_limit: ${ES_MEM_LIMIT}
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s --cacert config/certs/ca/ca.crt http://localhost:9200 | grep -q 'missing authentication credentials'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120

  kibana:
    depends_on:
      es01:
        condition: service_healthy
    image: docker.elastic.co/kibana/kibana:${STACK_VERSION}
    hostname: kibana
    labels:
      co.elastic.logs/module: kibana
    volumes:
      - kibanadata:/usr/share/kibana/data
      - ./config/kibana.yml:/usr/share/kibana/config/kibana.yml:ro
    ports:
      - ${KIBANA_PORT}:5601
    environment:
      - SERVERNAME=kibana
      - ELASTICSEARCH_HOSTS=http://es01:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}
      - XPACK_SECURITY_ENCRYPTIONKEY=${ENCRYPTION_KEY}
      - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=${ENCRYPTION_KEY}
      - XPACK_REPORTING_ENCRYPTIONKEY=${ENCRYPTION_KEY}
      - XPACK_REPORTING_KIBANASERVER_HOSTNAME=localhost
      - SERVER_SSL_ENABLED=false
    mem_limit: ${KB_MEM_LIMIT}
    healthcheck:
      test:
        [
           "CMD-SHELL",
          "curl -s -I --cacert config/certs/ca/ca.crt http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120

  # Docker container for running the configured fleet agent. The agent will capture docker logs
  fleet-server:
    depends_on:
      kibana:
        condition: service_healthy
      es01:
        condition: service_healthy
    image: docker.elastic.co/beats/elastic-agent:${STACK_VERSION}
    profiles:
      - elastic
    volumes:
      - "fleetserverdata:/usr/share/elastic-agent"
      - "/var/lib/docker/containers:/var/lib/docker/containers:ro"
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
      - "/sys/fs/cgroup:/hostfs/sys/fs/cgroup:ro"
      - "/proc:/hostfs/proc:ro"
      - "/:/hostfs:ro"
    ports:
      - ${FLEET_PORT}:8220
    user: root
    environment:
      - FLEET_ENROLL=1
      - FLEET_INSECURE=true
      - FLEET_SERVER_ELASTICSEARCH_HOST=http://es01:9200
      - FLEET_SERVER_ELASTICSEARCH_INSECURE=true
      - FLEET_SERVER_ENABLE=1
      - FLEET_SERVER_INSECURE_HTTP=true
      - FLEET_SERVER_POLICY_ID=fleet-server-policy
      - FLEET_URL=http://fleet-server:8220
      - KIBANA_FLEET_SETUP=1
      - KIBANA_FLEET_USERNAME=elastic
      - KIBANA_FLEET_PASSWORD=${ELASTIC_PASSWORD}
      - KIBANA_HOST=http://kibana:5601
      - KAFKA_BOOTSTRAP_SERVER=${KAFKA_BOOTSTRAP_SERVER}
      - KAFKA_USER=${KAFKA_USER}
      - KAFKA_PASSWORD=${KAFKA_PASSWORD}
      - KAFKA_GROUP_ID=${KAFKA_GROUP_ID}
      - KAFKA_CLIENT_ID=${KAFKA_CLIENT_ID}
      - KAFKA_TOPIC=${KAFKA_TOPIC}

volumes:
  data-zookeeper-log:
    driver: local
  data-zookeeper-data:
    driver: local
  data-broker:
    driver: local
  data-zookeeper-auditlog-log:
    driver: local
  data-zookeeper-auditlog-data:
    driver: local
  data-broker-auditlog:
    driver: local
  esdata01:
    driver: local
  kibanadata:
    driver: local
  fleetserverdata:
    driver: local
