= Demo for configuring audit logging

This playground for experimenting with audit logging with a dedicated audit log cluster

DISCLAIMER: This project is for demonstration purposes only. Using the demo unmodified in production is highly discouraged. Use at your own risk.

== How to use
Start the docker containers

```shell
docker compose up -d
```

Optionally, you can enable a docker profile with brings an ELK stack, either by setting an environment variable:

```shell
export COMPOSE_PROFILES=elastic
docker compose up -d
```

Or by specifying the profile directly:

```shell
docker compose --profile elastic up -d
```

This will set up two clusters, each with one Zookeeper node and one Kafka broker. The main instance is used for regular data. The second instance is used to route audit log data from the first instance too.

The content of the folder `examples` is visible in the `broker` container under `/examples`:

```shell
docker compose exec broker ls -l /examples
```

List the topics in the cluster by running:

```shell
docker compose exec broker kafka-topics --command-config /clients/admin.conf --bootstrap-server broker:9092 --list
```


A few ACLs should have been set up. You can list them like this::

```shell
docker compose exec broker kafka-acls --command-config /clients/admin.conf --bootstrap-server broker:9092 --list
```

=== Audit log

The setup has created a separate audit log "cluster" and prepared a few topics in it.

You can list the topics of the audit log cluster like this:

```shell
docker compose exec broker kafka-topics --command-config /clients/admin-auditlog.conf --bootstrap-server broker-auditlog:9092 --list
```

The following topics are configured in the audit log cluster:

* confluent-audit-log-events: Default topic capturing all events
* confluent-audit-log-events-management: Capture all management events. Not used by our default setup in this demo.
* confluent-audit-log-events-describe: Capture all describe events. Not used by our default setup in this demo.
* confluent-audit-log-events-produce: Capture all produce events. Not used by our default setup in this demo.
* confluent-audit-log-events-consume: Capture all consume events. Not used by our default setup in this demo.

However, in the default configuration set up by the demo, only denied requests are shown in the logs.

In a separate shell, start consuming from the topic gathering audit log events for produce requests one:

```shell
docker compose exec broker-auditlog kafka-console-consumer --consumer.config /clients/admin-auditlog.conf --bootstrap-server broker-auditlog:9092 --topic confluent-audit-log-events
```

Produce something to the `test` topic in the main cluster, as authorization will be granted, nothing should show up in the log files:

```shell
docker compose exec broker kafka-console-producer --producer.config /clients/producer.conf --bootstrap-server broker:9092 --topic test
```

Now produce something but use the wrong credentials (the one created for the `consumer` are only allowed to read events). Authorization will be denied:

```shell
docker compose exec broker kafka-console-producer --producer.config /clients/consumer.conf --bootstrap-server broker:9092 --topic test
```

You will see many events in the audit log topic. If you use credentials allowed to produce to the topic, you won't see any audit log events as by default we only log rejected requests in this demo.

=== Dynamic configuration using `kafka-configs`

First and most important note: If you use Confluent Platform, please set up `mds` and use either the `confluent` cli tool or the REST API to create or update the dynamic configurations. Please do not use `kafka-configs` as this tool allows you to screw up your whole setup easily due to missing consistency checks.

Now we want to use dynamic configuration to temporarily increase the level of detail of what is logged to the audit log cluster.

First, let's show the dynamic configuration of the main cluster:

```shell
docker compose exec broker kafka-configs --command-config /clients/admin.conf --bootstrap-server broker:9092 --describe --entity-type brokers
```

Currently, we do not track successful requests. Let's change that, which might be useful for e.g. debugging.
Apply the dynamic configuration found in file `/examples/dynamic-configs/GoodAuditing.json`:

```shell
docker compose exec broker kafka-configs --command-config /clients/admin.conf --bootstrap-server broker:9092 --alter --broker-defaults --add-config-file /examples/dynamic-configs/GoodAuditing.json
```

Optionally, show the applied dynamic broker configuration:

```shell
docker compose exec broker kafka-configs --command-config /clients/admin.conf --bootstrap-server broker:9092 --broker-defaults --describe
```

Optionally, show all configurations for brokers (including the static configuration):

```shell
docker compose exec broker kafka-configs --command-config /clients/admin.conf --bootstrap-server broker:9092 --entity-type brokers --describe --all
```


Produce to the cluster in order to create some audit log events:

```shell
docker compose exec broker kafka-console-producer --producer.config /clients/producer.conf --bootstrap-server broker:9092 --topic test
```

Try producing with wrong credentials:

```shell
docker compose exec broker kafka-console-producer --producer.config /clients/consumer.conf --bootstrap-server broker:9092 --topic test
```

In both cases you should now see events in the audit log topic.

Optionally, you can consume the successfully produced events from the cluster like this:

```shell
docker compose exec broker kafka-console-consumer --consumer.config /clients/consumer.conf --bootstrap-server broker:9092 --topic test --from-beginning
```

Now, let's remove the dynamic configuration again:

```shell
docker compose exec broker kafka-configs --command-config /clients/admin.conf --bootstrap-server broker:9092 --alter --broker-defaults --delete-config confluent.security.event.router.config
```

== Optionally: Usage of Elastic, Kibana and Elastic agent

If you have enabled the `elastic` Docker profile, you can access `kibana` with your web browser here (you need to create an exception as the certificate is self-signed):

* URL: `http://localhost:5601`
* Username: `elastic`
* Password: `elastic`

On the left side, click on `Observability`, then open `Stream`. After a while, you should start to see Kafka Audit Log events (`event.dataset: kafka_log.generic`).

Go back to the main page and open `Analytics->Discover`. You should be able to see all the unpacked json events.
Use a filter such as `event.dataset : "kafka_log.generic"` to show only the audit log data.

Many more options are available. Note that the content of the `message` field of the Kafka event is extracted by the configuration automatically thus making all contained fields availablel too.


== Clean-up

Shut down all containers and remove all persistant data:

```shell
COMPOSE_PROFILES=elastic docker compose down -v
```

